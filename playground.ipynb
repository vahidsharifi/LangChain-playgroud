{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLangChain is a blockchain-based platform for learning languages through incentivized collaboration.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(tiktoken_model_name=\"text-davinci-003\")\n",
    "llm(\"explain what is LangChain in one sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    SystemMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "message = [\n",
    "    SystemMessage(content=\"You are an expert data scientist\"),\n",
    "    HumanMessage(content=\"Write a Python sctipt that run box-cox transformation on numerical columns of a dataframe.\")\n",
    "]\n",
    "response = chat(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a Python script that uses the `scipy` library to perform the Box-Cox transformation on numerical columns of a dataframe:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from scipy import stats\n",
      "\n",
      "def boxcox_transform(df):\n",
      "    # Create a new dataframe to store the transformed values\n",
      "    transformed_df = pd.DataFrame()\n",
      "\n",
      "    # Iterate over each column in the dataframe\n",
      "    for column in df.columns:\n",
      "        # Check if the column is numeric\n",
      "        if pd.api.types.is_numeric_dtype(df[column]):\n",
      "            # Apply the Box-Cox transformation\n",
      "            transformed_data, _ = stats.boxcox(df[column])\n",
      "\n",
      "            # Add the transformed values to the new dataframe\n",
      "            transformed_df[column] = transformed_data\n",
      "        else:\n",
      "            # If the column is not numeric, add it as it is to the new dataframe\n",
      "            transformed_df[column] = df[column]\n",
      "\n",
      "    return transformed_df\n",
      "\n",
      "# Example usage\n",
      "# Create a sample dataframe\n",
      "data = {'A': [1, 2, 3, 4, 5],\n",
      "        'B': [6, 7, 8, 9, 10],\n",
      "        'C': ['a', 'b', 'c', 'd', 'e']}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Apply the Box-Cox transformation on the numerical columns\n",
      "transformed_df = boxcox_transform(df)\n",
      "\n",
      "# Print the original and transformed dataframes\n",
      "print(\"Original dataframe:\")\n",
      "print(df)\n",
      "print(\"\\nTransformed dataframe:\")\n",
      "print(transformed_df)\n",
      "```\n",
      "\n",
      "In this script, the `boxcox_transform` function takes a dataframe as input and returns a new dataframe with the Box-Cox transformed values. It iterates over each column in the dataframe and checks if the column is numeric using the `is_numeric_dtype` function from the `pandas` library. If the column is numeric, it applies the `boxcox` transformation from the `scipy` library and adds the transformed values to the new dataframe. If the column is not numeric, it adds it as it is to the new dataframe. Finally, it returns the transformed dataframe.\n",
      "\n",
      "You can replace the example usage with your own dataframe to apply the Box-Cox transformation on your data.\n"
     ]
    }
   ],
   "source": [
    "print(response.content, end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert in psychology especially emotions. You get a negative emotion name and give advice to turn that emotion to happiness.\n",
    "Explain how to turn feeling {negative_emotion} to hapiness in three steps.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"negative_emotion\"],\n",
    "    template=template,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm(prompt.format(negative_emotion='anger'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Acknowledge Your Anger: The first step to turning anger into happiness is to acknowledge that you are feeling angry. Take a few moments to reflect on the situation that is causing your anger, and identify the thoughts, sensations, and behaviors that accompany the emotion.\n",
      "\n",
      "2. Reframe Your Perspective: After you have acknowledged your anger, it’s important to reframe your perspective on the situation. Remind yourself that even though it might feel like the situation is out of your control, you can choose how you respond to it. Take a moment to practice mindful breathing, and focus on creating positive thoughts about the situation.\n",
      "\n",
      "3. Take Action: Once you’ve acknowledged your anger and reframed your perspective, it’s time to take action to turn your anger into happiness. This can look different for everyone, but some examples include: writing down your feelings in a journal, engaging in relaxing activities like yoga or meditation, or talking to a friend for support. Taking action can help you process your emotions in a healthy way and eventually find happiness.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Connect with Others: Spend time with people who make you feel good and reach out to those you trust. Connecting with others can help you feel less isolated and can boost your mood.\n",
      "\n",
      "2. Find Meaningful Activities: Doing activities that are meaningful to you, such as volunteering, taking classes, or pursuing a hobby, can help you feel more connected and give you a sense of purpose.\n",
      "\n",
      "3. Practice Self-Care: Make sure to take care of your physical and mental health needs. Take time for yourself to do activities that make you feel good, such as reading, exercising, or listening to music.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the Chain only specifying the input variable\n",
    "print(chain.run(\"loneliness\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark for differenc SimpleSequencialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"negative_emotion\"],\n",
    "    template=\"Turn each step of {negative_emotion}, to three sub-steps.\",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "1. Identify the source of the frustration: Before trying to turn frustration into happiness, it is important to take a moment to identify what is causing the frustration. Is it a certain task or activity? A person or situation? Once the source of the frustration is identified, it can be easier to address the issue.\n",
      "\n",
      "2. Find a solution: Once the source of the frustration is identified, it is important to find a solution or action plan to resolve the issue. This might involve asking for help, doing research, or taking a break from the task. \n",
      "\n",
      "3. Take time to relax: Once a solution has been found, it is important to take time to relax and reflect on the situation. This might involve taking a few deep breaths, going for a walk, or doing something that brings joy. Taking time to relax will help to refocus the mind and shift from feeling frustrated to feeling happy.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "A. Identify the source of the frustration\n",
      "\n",
      "B. Find a solution\n",
      "\n",
      "C. Take time to relax\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "A. Identify the source of the frustration\n",
      "\n",
      "B. Find a solution\n",
      "\n",
      "C. Take time to relax\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the shain specifying only the input variable for the first chain\n",
    "explanation = overall_chain.run(\"fraustration\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "1. Take a Break: When feeling frustrated, it is important to take a short break to gather yourself and clear your mind. Step away from the situation for a few minutes and take deep breaths. This will help to reduce the intensity of the emotion.\n",
      "\n",
      "2. Change Your Perspective: Reframe the situation and look at it in a different way. Ask yourself what the positive aspects of the situation are and focus on that. This will help to shift your focus to the positive aspects of the situation.\n",
      "\n",
      "3. Take Action: Choose to act in a way that will bring about positive results. Take action to address the underlying cause of the frustration. Develop a plan of action and take small steps to make progress toward a positive outcome. This will help to reduce the feelings of frustration and create a sense of accomplishment.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "Sub-Step 1: Take a Break: Find a quiet space and take a few deep breaths to help clear your mind.\n",
      "\n",
      "Sub-Step 2: Change Your Perspective: Shift your focus to the positive aspects of the situation by asking yourself what the positive aspects are.\n",
      "\n",
      "Sub-Step 3: Take Action: Develop a plan of action to address the underlying cause of the frustration and take small steps to make progress.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Sub-Step 1: Take a Break: Find a quiet space and take a few deep breaths to help clear your mind.\n",
      "\n",
      "Sub-Step 2: Change Your Perspective: Shift your focus to the positive aspects of the situation by asking yourself what the positive aspects are.\n",
      "\n",
      "Sub-Step 3: Take Action: Develop a plan of action to address the underlying cause of the frustration and take small steps to make progress.\n"
     ]
    }
   ],
   "source": [
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"negative_emotion\"],\n",
    "    template=\"Turn each step of three steps for changing {negative_emotion} to hapiness, to three sub-steps.\",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the shain specifying only the input variable for the first chain\n",
    "explanation = overall_chain.run(\"fraustration\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same prompt but to check the difference between each response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "1. Take a break: Step away from the situation that is causing your frustration and take a few moments for yourself to recenter. This could be as simple as stepping outside for a few minutes or going for a walk.\n",
      "\n",
      "2. Reframe your thoughts: Observe your thoughts and try to reframe them in a more positive light. Instead of feeling overwhelmed, focus on the progress you’ve made and how far you’ve come.\n",
      "\n",
      "3. Practice gratitude: Take a moment to be grateful for something in your life. This could be anything from the people around you to the sunshine in the sky. Gratitude helps to put our lives into perspective and can help to shift our emotions from frustration to happiness.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "Sub-Step 1: Step away from the situation and take a few moments for yourself.\n",
      "\n",
      "Sub-Step 2: Observe your thoughts and reframe them into a more positive light.\n",
      "\n",
      "Sub-Step 3: Take a moment to be grateful for something in your life.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Sub-Step 1: Step away from the situation and take a few moments for yourself.\n",
      "\n",
      "Sub-Step 2: Observe your thoughts and reframe them into a more positive light.\n",
      "\n",
      "Sub-Step 3: Take a moment to be grateful for something in your life.\n"
     ]
    }
   ],
   "source": [
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"negative_emotion\"],\n",
    "    template=\"Turn each step of three steps for changing {negative_emotion} to hapiness, to three sub-steps.\",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "\n",
    "# Run the shain specifying only the input variable for the first chain\n",
    "explanation = overall_chain.run(\"fraustration\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example to understand how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Frustration\n",
      "2. Resentment\n",
      "3. Hostility\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert in psychology especially emotions. You get a negative emotion name and return three other emotions name which usually comes from with it.\n",
    "List the name of three emotions comes with {emotion}.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"emotion\"],\n",
    "    template=template,\n",
    "\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the Chain only specifying the input variable\n",
    "print(chain.run(\"anger\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"emotions_list\"],\n",
    "    template=\"Explain each of given emotions in two lines. The emotions are {emotions_list}.\",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "1. Frustration\n",
      "2. Resentment\n",
      "3. Disgust\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "1. Frustration: Frustration is an emotion characterized by feelings of anger, disappointment, and dissatisfaction due to an inability to achieve a desired outcome.\n",
      "\n",
      "2. Resentment: Resentment is an emotion characterized by a strong feeling of displeasure or indignation caused by a perceived wrong or insult.\n",
      "\n",
      "3. Disgust: Disgust is an emotion characterized by an aversion to something considered unpleasant, offensive, or repulsive.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "1. Frustration: Frustration is an emotion characterized by feelings of anger, disappointment, and dissatisfaction due to an inability to achieve a desired outcome.\n",
      "\n",
      "2. Resentment: Resentment is an emotion characterized by a strong feeling of displeasure or indignation caused by a perceived wrong or insult.\n",
      "\n",
      "3. Disgust: Disgust is an emotion characterized by an aversion to something considered unpleasant, offensive, or repulsive.\n"
     ]
    }
   ],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the shain specifying only the input variable for the first chain\n",
    "explanation = overall_chain.run(\"anger\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "NLP (Natural Language Processing) is a field of artificial intelligence that deals with the analysis and understanding of human language. It includes tasks such as sentiment analysis, text classification, entity recognition, and machine translation. NLP enables machines to understand and interpret human language in order to process and generate meaningful responses.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "NLP stands for Natural Language Processing. It's a way for computers to understand what people say. Imagine you have a robot friend at home. You'd like him to understand what you're saying and be able to respond. That's where NLP comes in.\n",
      "\n",
      "NLP is a branch of Artificial Intelligence (AI) that allows computers to read, interpret, and understand human language. For example, when you type into a search engine, the search engine uses NLP to understand what you're looking for.\n",
      "\n",
      "NLP is like teaching your robot friend a whole new language. The first step is to teach the robot how to break down sentences. The robot would need to learn how to recognize words and how to group them into phrases and sentences. Once the robot understands the structure of sentences, it can start to learn what each word means.\n",
      "\n",
      "In order to understand the meaning behind a sentence, the robot would need to understand the context. For example, if you said “I'm hungry”, the robot would need to understand that you want food. To help the robot understand context, NLP uses machine learning algorithms to analyze large amounts of data.\n",
      "\n",
      "Once the robot has learned how to break down sentences and understand context,\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "NLP stands for Natural Language Processing. It's a way for computers to understand what people say. Imagine you have a robot friend at home. You'd like him to understand what you're saying and be able to respond. That's where NLP comes in.\n",
      "\n",
      "NLP is a branch of Artificial Intelligence (AI) that allows computers to read, interpret, and understand human language. For example, when you type into a search engine, the search engine uses NLP to understand what you're looking for.\n",
      "\n",
      "NLP is like teaching your robot friend a whole new language. The first step is to teach the robot how to break down sentences. The robot would need to learn how to recognize words and how to group them into phrases and sentences. Once the robot understands the structure of sentences, it can start to learn what each word means.\n",
      "\n",
      "In order to understand the meaning behind a sentence, the robot would need to understand the context. For example, if you said “I'm hungry”, the robot would need to understand that you want food. To help the robot understand context, NLP uses machine learning algorithms to analyze large amounts of data.\n",
      "\n",
      "Once the robot has learned how to break down sentences and understand context,\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You are an expert data scientist.\n",
    "Explain {topic} in three lines.\n",
    "\"\"\"\n",
    "\n",
    "first_prompt = PromptTemplate(\n",
    "    input_variables=['topic'],\n",
    "    template=template,\n",
    "\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"dsc\"],\n",
    "    template=\" Turn the the concept discription of {dsc} and explain it to me like I am five in 500 words\",\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "\n",
    "\n",
    "# third_prompt = PromptTemplate(\n",
    "#     input_variables=[\"topics\"],\n",
    "#     template=\"Explain the given topics in two line. Topics: {topics} \",\n",
    "# )\n",
    "# chain_three = LLMChain(llm=llm, prompt=third_prompt)\n",
    "\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n",
    "\n",
    "# Run the shain specifying only the input variable for the first chain\n",
    "explanation = overall_chain.run(\"NLP\")\n",
    "print(explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([explanation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"NLP stands for Natural Language Processing. It's a way for computers to understand what people say.\", metadata={}),\n",
       " Document(page_content=\"Imagine you have a robot friend at home. You'd like him to understand what you're saying and be\", metadata={}),\n",
       " Document(page_content=\"able to respond. That's where NLP comes in.\", metadata={}),\n",
       " Document(page_content='NLP is a branch of Artificial Intelligence (AI) that allows computers to read, interpret, and', metadata={}),\n",
       " Document(page_content='understand human language. For example, when you type into a search engine, the search engine uses', metadata={}),\n",
       " Document(page_content=\"NLP to understand what you're looking for.\", metadata={}),\n",
       " Document(page_content='NLP is like teaching your robot friend a whole new language. The first step is to teach the robot', metadata={}),\n",
       " Document(page_content='how to break down sentences. The robot would need to learn how to recognize words and how to group', metadata={}),\n",
       " Document(page_content='them into phrases and sentences. Once the robot understands the structure of sentences, it can', metadata={}),\n",
       " Document(page_content='start to learn what each word means.', metadata={}),\n",
       " Document(page_content='In order to understand the meaning behind a sentence, the robot would need to understand the', metadata={}),\n",
       " Document(page_content=\"context. For example, if you said “I'm hungry”, the robot would need to understand that you want\", metadata={}),\n",
       " Document(page_content='food. To help the robot understand context, NLP uses machine learning algorithms to analyze large', metadata={}),\n",
       " Document(page_content='amounts of data.', metadata={}),\n",
       " Document(page_content='Once the robot has learned how to break down sentences and understand context,', metadata={})]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"NLP stands for Natural Language Processing. It's a way for computers to understand what people say.\""
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='ada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "You are not allowed to generate embeddings from this model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query_result \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39;49membed_query(texts[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mpage_content)\n\u001b[1;32m      2\u001b[0m query_result\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:536\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed_query\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    528\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \n\u001b[1;32m    530\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39m        Embedding for the text.\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 536\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_func(text, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n\u001b[1;32m    537\u001b[0m     \u001b[39mreturn\u001b[39;00m embedding\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:467\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._embedding_func\u001b[0;34m(self, text, engine)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m001\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    464\u001b[0m     \u001b[39m# See: https://github.com/openai/openai-python/issues/418#issuecomment-1525939500\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[39m# replace newlines, which can negatively affect performance.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 467\u001b[0m \u001b[39mreturn\u001b[39;00m embed_with_retry(\n\u001b[1;32m    468\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    469\u001b[0m     \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m[text],\n\u001b[1;32m    470\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_invocation_params,\n\u001b[1;32m    471\u001b[0m )[\n\u001b[1;32m    472\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m ][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:108\u001b[0m, in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     response \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m _check_response(response)\n\u001b[0;32m--> 108\u001b[0m \u001b[39mreturn\u001b[39;00m _embed_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:105\u001b[0m, in \u001b[0;36membed_with_retry.<locals>._embed_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_embed_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 105\u001b[0m     response \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m _check_response(response)\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/openai/api_resources/embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     35\u001b[0m         \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         \u001b[39m# This is only for the default case.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m user_provided_encoding_format:\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mPermissionError\u001b[0m: You are not allowed to generate embeddings from this model"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(texts[0].page_content)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing in pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "# initialise pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),\n",
    "    envoronment=os.getenv('PINECONE_ENV')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No active indexes found in your Pinecone project, are you sure you're using the right API key and environment?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m index_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlangchain-quickstart\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m search \u001b[39m=\u001b[39m Pinecone\u001b[39m.\u001b[39;49mfrom_documents(texts, embeddings, index_name\u001b[39m=\u001b[39;49mindex_name)\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/langchain/vectorstores/base.py:402\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    401\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m--> 402\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39;49mmetadatas, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/GitHub/LangChain-playgroud/venv/lib/python3.11/site-packages/langchain/vectorstores/pinecone.py:320\u001b[0m, in \u001b[0;36mPinecone.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, batch_size, text_key, index_name, namespace, upsert_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     index \u001b[39m=\u001b[39m pinecone\u001b[39m.\u001b[39mIndex(index_name)\n\u001b[1;32m    319\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(indexes) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    321\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo active indexes found in your Pinecone project, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mare you sure you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre using the right API key and environment?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    323\u001b[0m     )\n\u001b[1;32m    324\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    326\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIndex \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mindex_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not found in your Pinecone project. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDid you mean one of the following indexes: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(indexes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No active indexes found in your Pinecone project, are you sure you're using the right API key and environment?"
     ]
    }
   ],
   "source": [
    "index_name = 'langchain-quickstart'\n",
    "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat is the first step to teach the robot?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m result \u001b[39m=\u001b[39m search\u001b[39m.\u001b[39msimilarity_search(query)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'search' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"What is the first step to teach the robot?\"\n",
    "result = search.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/integrations/document_loaders/web_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36132"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader.load()[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
